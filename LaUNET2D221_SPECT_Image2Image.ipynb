{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuhiRaj/3DSimulations/blob/main/LaUNET2D221_SPECT_Image2Image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qo1yhMEfhkn"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOYnLA6ez_IV",
        "outputId": "49569eac-e16b-4d21-caf3-d35b5b6f0390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x10 size: torch.Size([1, 512, 31, 31])\n",
            "x7 size: torch.Size([1, 512, 31, 31])\n",
            "Concatenated size: torch.Size([1, 1024, 31, 31])\n",
            "1 torch.Size([1, 1, 250, 250])\n",
            "+----------------------+------------+\n",
            "|       Modules        | Parameters |\n",
            "+----------------------+------------+\n",
            "| down_conv_1.0.weight |    576     |\n",
            "|  down_conv_1.0.bias  |     64     |\n",
            "| down_conv_1.2.weight |   36864    |\n",
            "|  down_conv_1.2.bias  |     64     |\n",
            "| down_conv_2.0.weight |   73728    |\n",
            "|  down_conv_2.0.bias  |    128     |\n",
            "| down_conv_2.2.weight |   147456   |\n",
            "|  down_conv_2.2.bias  |    128     |\n",
            "| down_conv_3.0.weight |   294912   |\n",
            "|  down_conv_3.0.bias  |    256     |\n",
            "| down_conv_3.2.weight |   589824   |\n",
            "|  down_conv_3.2.bias  |    256     |\n",
            "| down_conv_4.0.weight |  1179648   |\n",
            "|  down_conv_4.0.bias  |    512     |\n",
            "| down_conv_4.2.weight |  2359296   |\n",
            "|  down_conv_4.2.bias  |    512     |\n",
            "| down_conv_5.0.weight |  4718592   |\n",
            "|  down_conv_5.0.bias  |    1024    |\n",
            "| down_conv_5.2.weight |  9437184   |\n",
            "|  down_conv_5.2.bias  |    1024    |\n",
            "|  up_trans_1.weight   |  2097152   |\n",
            "|   up_trans_1.bias    |    512     |\n",
            "|  up_conv_1.0.weight  |  4718592   |\n",
            "|   up_conv_1.0.bias   |    512     |\n",
            "|  up_conv_1.2.weight  |  2359296   |\n",
            "|   up_conv_1.2.bias   |    512     |\n",
            "|  up_trans_2.weight   |   524288   |\n",
            "|   up_trans_2.bias    |    256     |\n",
            "|  up_conv_2.0.weight  |  1179648   |\n",
            "|   up_conv_2.0.bias   |    256     |\n",
            "|  up_conv_2.2.weight  |   589824   |\n",
            "|   up_conv_2.2.bias   |    256     |\n",
            "|  up_trans_3.weight   |   131072   |\n",
            "|   up_trans_3.bias    |    128     |\n",
            "|  up_conv_3.0.weight  |   294912   |\n",
            "|   up_conv_3.0.bias   |    128     |\n",
            "|  up_conv_3.2.weight  |   147456   |\n",
            "|   up_conv_3.2.bias   |    128     |\n",
            "|  up_trans_4.weight   |   32768    |\n",
            "|   up_trans_4.bias    |     64     |\n",
            "|  up_conv_4.0.weight  |   73728    |\n",
            "|   up_conv_4.0.bias   |     64     |\n",
            "|  up_conv_4.2.weight  |   36864    |\n",
            "|   up_conv_4.2.bias   |     64     |\n",
            "|      out.weight      |     64     |\n",
            "|       out.bias       |     1      |\n",
            "+----------------------+------------+\n",
            "Total Trainable Params: 31030593\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31030593"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from IPython.core.display import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "def double_conv(in_c, out_c):\n",
        "  conv = nn.Sequential(\n",
        "      nn.Conv2d(in_c, out_c, kernel_size=3, padding=\"same\"),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(out_c, out_c, kernel_size=3, padding=\"same\"),\n",
        "      nn.ReLU(inplace=True),\n",
        "  )\n",
        "  return conv\n",
        "\n",
        "def double_conv_up(in_c, out_c):\n",
        "  conv = nn.Sequential(\n",
        "      nn.Conv2d(in_c, out_c, kernel_size=3, padding=\"same\"),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(out_c, out_c, kernel_size=3, padding=\"same\"),\n",
        "      nn.ReLU(inplace=True),\n",
        "  )\n",
        "  return conv\n",
        "\n",
        "\n",
        "def crop_img(tensor, target_tensor):\n",
        "    target_size = target_tensor.size()[2]\n",
        "\n",
        "    # Calculate the difference in sizes\n",
        "    delta_h = tensor.size()[2] - target_size\n",
        "    delta_w = tensor.size()[3] - target_size\n",
        "\n",
        "    # Calculate cropping indices\n",
        "    h_start, h_end = delta_h // 2, tensor.size()[2] - delta_h // 2\n",
        "    w_start, w_end = delta_w // 2, tensor.size()[3] - delta_w // 2\n",
        "\n",
        "    return tensor[:, :, h_start:h_end, w_start:w_end]\n",
        "\n",
        "\n",
        "'''\n",
        "def crop_img(tensor, target_tensor):\n",
        "  target_size = target_tensor.size()[2]\n",
        "  if tensor.size()[2] % 2 == 1:\n",
        "    tensor_size = tensor.size()[2]-1\n",
        "  else:\n",
        "    tensor_size = tensor.size()[2]\n",
        "  delta = tensor_size - target_size\n",
        "  delta = delta // 2\n",
        "  return tensor[:, :, delta:tensor_size-delta, delta:tensor_size-delta]\n",
        "'''\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(UNet, self).__init__()\n",
        "    self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.down_conv_1 = double_conv(1,64)\n",
        "    self.down_conv_2 = double_conv(64,128)\n",
        "    self.down_conv_3 = double_conv(128,256)\n",
        "    self.down_conv_4 = double_conv(256,512)\n",
        "    self.down_conv_5 = double_conv(512,1024)\n",
        "\n",
        "\n",
        "\n",
        "    self.up_trans_1 = nn.ConvTranspose2d(in_channels=1024,\n",
        "                                         out_channels=512,\n",
        "                                         kernel_size=2,\n",
        "                                         stride=2,\n",
        "                                         output_padding=1)\n",
        "\n",
        "    self.up_conv_1 = double_conv_up(1024, 512)\n",
        "\n",
        "\n",
        "\n",
        "    self.up_trans_2 = nn.ConvTranspose2d(in_channels=512,\n",
        "                                         out_channels=256,\n",
        "                                         kernel_size=2,\n",
        "                                         stride=2,\n",
        "                                         output_padding=0)\n",
        "\n",
        "    self.up_conv_2 = double_conv_up(512, 256)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    self.up_trans_3 = nn.ConvTranspose2d(in_channels=256,\n",
        "                                     out_channels=128,\n",
        "                                     kernel_size=2,\n",
        "                                     stride=2,\n",
        "                                     output_padding=1)  # Adjusted output_padding\n",
        "\n",
        "\n",
        "    self.up_conv_3 = double_conv_up(256, 128)\n",
        "\n",
        "\n",
        "\n",
        "    self.up_trans_4 = nn.ConvTranspose2d(in_channels=128,\n",
        "                                     out_channels=64,\n",
        "                                     kernel_size=2,\n",
        "                                     stride=2,\n",
        "                                     output_padding=0)  # Change this line\n",
        "\n",
        "    self.up_conv_4 = double_conv_up(128, 64)\n",
        "\n",
        "    self.out = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=1,\n",
        "        kernel_size=1,\n",
        "        stride=1\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, image):\n",
        "    # bs, c, h, w\n",
        "    #encoder\n",
        "    maxx = torch.mean(image)\n",
        "    x1 = self.down_conv_1(image/maxx) #A1\n",
        "    x2 = self.max_pool_2x2(x1) #A2\n",
        "\n",
        "    x3 = self.down_conv_2(x2) #B1\n",
        "    x4 = self.max_pool_2x2(x3) #B2\n",
        "\n",
        "    x5 = self.down_conv_3(x4) #C1\n",
        "    x6 = self.max_pool_2x2(x5) #C2\n",
        "\n",
        "    x7 = self.down_conv_4(x6) #D1\n",
        "    x8 = self.max_pool_2x2(x7) #D2\n",
        "\n",
        "    #print('Bottle Neck')\n",
        "    x9 = self.down_conv_5(x8) #E1 (Neck of the UNet)\n",
        "\n",
        "    #decoder\n",
        "    x10 = self.up_trans_1(x9)\n",
        "    print(\"x10 size:\", x10.size())\n",
        "    print(\"x7 size:\", x7.size())\n",
        "    c1_cat = torch.cat([x10, x7], 1)\n",
        "    print(\"Concatenated size:\", c1_cat.size())\n",
        "\n",
        "    '''\n",
        "    #decoder\n",
        "    x10 = self.up_trans_1(x9)\n",
        "    c1_cat = torch.cat([x10, x7], 1)\n",
        "    '''\n",
        "\n",
        "    x11 = self.up_conv_1(c1_cat)\n",
        "    x12 = self.up_trans_2(x11)\n",
        "    c2_cat = torch.cat([x12, x5], 1)\n",
        "\n",
        "    x13 = self.up_conv_2(c2_cat)\n",
        "    x14 = self.up_trans_3(x13)\n",
        "    c3_cat = torch.cat([x14, x3], 1)\n",
        "\n",
        "    x15 = self.up_conv_3(c3_cat)\n",
        "    x16 = self.up_trans_4(x15)\n",
        "    c4_cat = torch.cat([x16, x1], 1)\n",
        "\n",
        "    x17 = self.up_conv_4(c4_cat)\n",
        "    x18 = self.out(x17)*maxx\n",
        "    return x18 + image\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  image = torch.rand((1, 1, 250, 250))\n",
        "  model = UNet()\n",
        "  print('1',model(image).shape)\n",
        "  #print(model)\n",
        "\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params+=params\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "count_parameters(model)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1evpS88eA2yJZ8FbxSlhVjJXBP6vIKazq",
      "authorship_tag": "ABX9TyPLZVEiFqQPo0BsDfJl4kzM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}